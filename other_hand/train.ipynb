{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1272, 30, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\n",
    "    'come',\n",
    "    'away',\n",
    "    'spin'\n",
    "]\n",
    "\n",
    "data = np.concatenate([\n",
    "    np.load('dataset/seq_come_1627646273.npy'),\n",
    "    np.load('dataset/seq_away_1627646273.npy'),\n",
    "    np.load('dataset/seq_spin_1627646273.npy')\n",
    "], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272, 30, 99)\n",
      "(1272,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1272, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1144, 30, 99) (1144, 3)\n",
      "(128, 30, 99) (128, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                41984     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 44,163\n",
      "Trainable params: 44,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/36 [==============================] - 2s 32ms/step - loss: 36.4824 - acc: 0.4326 - val_loss: 11.4115 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64062, saving model to models\\model.h5\n",
      "Epoch 2/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 9.2537 - acc: 0.5926 - val_loss: 5.5519 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.64062 to 0.67969, saving model to models\\model.h5\n",
      "Epoch 3/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.5168 - acc: 0.5947 - val_loss: 1.0785 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67969 to 0.81250, saving model to models\\model.h5\n",
      "Epoch 4/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.6389 - acc: 0.8504 - val_loss: 0.2597 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.81250 to 0.91406, saving model to models\\model.h5\n",
      "Epoch 5/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.1893 - acc: 0.9419 - val_loss: 0.2801 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.91406 to 0.94531, saving model to models\\model.h5\n",
      "Epoch 6/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0993 - acc: 0.9711 - val_loss: 0.2228 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.94531 to 0.96875, saving model to models\\model.h5\n",
      "Epoch 7/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0916 - acc: 0.9780 - val_loss: 0.1837 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.96875\n",
      "Epoch 8/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0398 - acc: 0.9894 - val_loss: 0.1385 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.96875\n",
      "Epoch 9/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0631 - acc: 0.9858 - val_loss: 0.1460 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.96875\n",
      "Epoch 10/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.1112 - acc: 0.9782 - val_loss: 0.1120 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.96875 to 0.98438, saving model to models\\model.h5\n",
      "Epoch 11/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0888 - acc: 0.9796 - val_loss: 0.1448 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.98438\n",
      "Epoch 12/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0420 - acc: 0.9954 - val_loss: 0.0792 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.98438\n",
      "Epoch 13/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0150 - acc: 0.9967 - val_loss: 0.0908 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.98438\n",
      "Epoch 14/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0313 - acc: 0.9963 - val_loss: 0.0691 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.98438\n",
      "Epoch 15/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0182 - acc: 0.9971 - val_loss: 0.0264 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.98438 to 0.99219, saving model to models\\model.h5\n",
      "Epoch 16/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0044 - acc: 0.9996 - val_loss: 0.1087 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.99219\n",
      "Epoch 17/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0061 - acc: 0.9992 - val_loss: 0.1210 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.99219\n",
      "Epoch 18/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0051 - acc: 0.9996 - val_loss: 0.0175 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.99219\n",
      "Epoch 19/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 6.2550e-04 - acc: 1.0000 - val_loss: 0.0252 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.99219\n",
      "Epoch 20/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.7884e-04 - acc: 1.0000 - val_loss: 0.0218 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.99219\n",
      "Epoch 21/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.9796e-04 - acc: 1.0000 - val_loss: 0.0208 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.99219\n",
      "Epoch 22/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.6001e-04 - acc: 1.0000 - val_loss: 0.0201 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.99219\n",
      "Epoch 23/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.8332e-04 - acc: 1.0000 - val_loss: 0.0196 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.99219\n",
      "Epoch 24/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.0450e-04 - acc: 1.0000 - val_loss: 0.0199 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.99219\n",
      "Epoch 25/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.0936e-04 - acc: 1.0000 - val_loss: 0.0193 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.99219\n",
      "Epoch 26/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.1227e-04 - acc: 1.0000 - val_loss: 0.0181 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.99219\n",
      "Epoch 27/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.0633e-04 - acc: 1.0000 - val_loss: 0.0187 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.99219\n",
      "Epoch 28/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.8112e-04 - acc: 1.0000 - val_loss: 0.0184 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.99219\n",
      "Epoch 29/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.2924e-04 - acc: 1.0000 - val_loss: 0.0190 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.99219\n",
      "Epoch 30/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.0663e-04 - acc: 1.0000 - val_loss: 0.0178 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.99219\n",
      "Epoch 31/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.2086e-04 - acc: 1.0000 - val_loss: 0.0175 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.99219\n",
      "Epoch 32/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.6353e-04 - acc: 1.0000 - val_loss: 0.0168 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.99219\n",
      "Epoch 33/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.5266e-04 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.99219\n",
      "Epoch 34/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.5301e-04 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.99219\n",
      "Epoch 35/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.5371e-04 - acc: 1.0000 - val_loss: 0.0162 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.99219\n",
      "Epoch 36/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 2.1598e-04 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.99219\n",
      "Epoch 37/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.3273e-04 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.99219\n",
      "Epoch 38/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6311e-04 - acc: 1.0000 - val_loss: 0.0151 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.99219\n",
      "Epoch 39/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.3099e-04 - acc: 1.0000 - val_loss: 0.0150 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.99219\n",
      "Epoch 40/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5468e-04 - acc: 1.0000 - val_loss: 0.0152 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.99219\n",
      "Epoch 41/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.8820e-04 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.99219\n",
      "Epoch 42/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.3622e-04 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.99219\n",
      "Epoch 43/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7593e-04 - acc: 1.0000 - val_loss: 0.0143 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.99219\n",
      "Epoch 44/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5609e-04 - acc: 1.0000 - val_loss: 0.0136 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.99219\n",
      "Epoch 45/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.8287e-04 - acc: 1.0000 - val_loss: 0.0143 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.99219\n",
      "Epoch 46/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.2909e-04 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.99219\n",
      "Epoch 47/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7694e-04 - acc: 1.0000 - val_loss: 0.0134 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.99219\n",
      "Epoch 48/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.8104e-04 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.99219\n",
      "Epoch 49/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2389e-04 - acc: 1.0000 - val_loss: 0.0134 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.99219\n",
      "Epoch 50/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6242e-04 - acc: 1.0000 - val_loss: 0.0125 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.99219\n",
      "Epoch 51/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.4265e-04 - acc: 1.0000 - val_loss: 0.0130 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.99219\n",
      "Epoch 52/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2873e-04 - acc: 1.0000 - val_loss: 0.0127 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.99219\n",
      "Epoch 53/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7166e-04 - acc: 1.0000 - val_loss: 0.0124 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.99219\n",
      "Epoch 54/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2066e-04 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.99219\n",
      "Epoch 55/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 9.7123e-05 - acc: 1.0000 - val_loss: 0.0123 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.99219\n",
      "Epoch 56/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.2161e-04 - acc: 1.0000 - val_loss: 0.0127 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.99219\n",
      "Epoch 57/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.1492e-04 - acc: 1.0000 - val_loss: 0.0125 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.99219\n",
      "Epoch 58/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.5461e-04 - acc: 1.0000 - val_loss: 0.0123 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.99219\n",
      "Epoch 59/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.0640e-04 - acc: 1.0000 - val_loss: 0.0124 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.99219\n",
      "Epoch 60/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.1128e-04 - acc: 1.0000 - val_loss: 0.0116 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.99219\n",
      "Epoch 61/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.3543e-04 - acc: 1.0000 - val_loss: 0.0114 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.99219\n",
      "Epoch 62/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 1.0076e-04 - acc: 1.0000 - val_loss: 0.0112 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.99219\n",
      "Epoch 63/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 9.1596e-05 - acc: 1.0000 - val_loss: 0.0115 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.99219\n",
      "Epoch 64/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 7.2645e-05 - acc: 1.0000 - val_loss: 0.0111 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.99219\n",
      "Epoch 65/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 8.1295e-05 - acc: 1.0000 - val_loss: 0.0110 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.99219\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 66/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 7.8361e-05 - acc: 1.0000 - val_loss: 0.0109 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.99219\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.0340e-04 - acc: 1.0000 - val_loss: 0.0108 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.99219\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - ETA: 0s - loss: 7.8052e-05 - acc: 1.000 - 0s 10ms/step - loss: 7.9945e-05 - acc: 1.0000 - val_loss: 0.0106 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.99219\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 7.6432e-05 - acc: 1.0000 - val_loss: 0.0107 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.99219\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 7.0319e-05 - acc: 1.0000 - val_loss: 0.0105 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.99219\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 9.6070e-05 - acc: 1.0000 - val_loss: 0.0105 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.99219\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 8.4615e-05 - acc: 1.0000 - val_loss: 0.0104 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.99219\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 7.8130e-05 - acc: 1.0000 - val_loss: 0.0102 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.99219\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 7.9988e-05 - acc: 1.0000 - val_loss: 0.0103 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.99219\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.0622e-04 - acc: 1.0000 - val_loss: 0.0101 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.99219\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 6.0228e-05 - acc: 1.0000 - val_loss: 0.0102 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.99219\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 6.2244e-05 - acc: 1.0000 - val_loss: 0.0103 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.99219\n",
      "Epoch 78/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 5.9853e-05 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.99219\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 6.3708e-05 - acc: 1.0000 - val_loss: 0.0101 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.99219\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 7.5261e-05 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.99219\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 7.3626e-05 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.99219\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.3322e-05 - acc: 1.0000 - val_loss: 0.0097 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.99219\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.9015e-05 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.99219\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.0463e-04 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.99219\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 6.3314e-05 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.99219\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 7.5923e-05 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.99219\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 6.3030e-05 - acc: 1.0000 - val_loss: 0.0093 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.99219\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 5.3587e-05 - acc: 1.0000 - val_loss: 0.0093 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.99219\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 5.7610e-05 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.99219\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 6.2095e-05 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.99219\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 7.3577e-05 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.99219\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.0464e-05 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.99219\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 6.5816e-05 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.99219\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 4.9422e-05 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.99219\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 4.2503e-05 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00095: val_acc improved from 0.99219 to 1.00000, saving model to models\\model.h5\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 7.7956e-05 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 1.00000\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.9873e-05 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 1.00000\n",
      "Epoch 98/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.5786e-05 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 1.00000\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.5170e-05 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 1.00000\n",
      "Epoch 100/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.8164e-05 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 1.00000\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.2962e-05 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 1.00000\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.2551e-05 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 1.00000\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.9044e-05 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 1.00000\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 6.0198e-05 - acc: 1.0000 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 1.00000\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.8723e-05 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 1.00000\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 6.3393e-05 - acc: 1.0000 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 1.00000\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.8025e-05 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 1.00000\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 5.8428e-05 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 1.00000\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 7.2770e-05 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 1.00000\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.3642e-05 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 1.00000\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.2683e-05 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 1.00000\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 6.2042e-05 - acc: 1.0000 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 1.00000\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 6.0590e-05 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 1.00000\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.7040e-05 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 1.00000\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.8447e-05 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 1.00000\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 5.1817e-05 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 1.00000\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.4499e-05 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 1.00000\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 6.1363e-05 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 1.00000\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 3.4990e-05 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 1.00000\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.7411e-05 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 1.00000\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 4.0862e-05 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 1.00000\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.6825e-05 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 1.00000\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.5624e-05 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 1.00000\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.4565e-05 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 1.00000\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.5744e-05 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 1.00000\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.9602e-05 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 1.00000\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.1416e-05 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 1.00000\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.3550e-05 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 1.00000\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.2009e-05 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 1.00000\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.6716e-05 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 1.00000\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.7577e-05 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 1.00000\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.7630e-05 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 1.00000\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 4.3186e-05 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 1.00000\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.5075e-05 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 1.00000\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.4950e-05 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 1.00000\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.3817e-05 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 1.00000\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.8798e-05 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 1.00000\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.4905e-05 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 1.00000\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.8271e-05 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 1.00000\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.5001e-05 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 1.00000\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.8698e-05 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 1.00000\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.0068e-05 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 1.00000\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.5679e-05 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 1.00000\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.7483e-05 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 1.00000\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.3924e-05 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.8434e-05 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 1.00000\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 2.9931e-05 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 1.00000\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.7872e-05 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 1.00000\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.7367e-05 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 1.00000\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.9982e-05 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 1.00000\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.0754e-05 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 1.00000\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.0839e-05 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 1.00000\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.7068e-05 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 1.00000\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.3530e-05 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 1.00000\n",
      "Epoch 155/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.8025e-05 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 1.00000\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.2595e-05 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 1.00000\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.9288e-05 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 1.00000\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.7691e-05 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 1.00000\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.7373e-05 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 1.00000\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.2866e-05 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 1.00000\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.9078e-05 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 1.00000\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.1332e-05 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 1.00000\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.7703e-05 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 1.00000\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.0679e-05 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 1.00000\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.2484e-05 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 1.00000\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.8467e-05 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 1.00000\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.4787e-05 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 1.00000\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.2055e-05 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 1.00000\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.2320e-05 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 1.00000\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.8125e-05 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 1.00000\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.0175e-05 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 1.00000\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.9383e-05 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 1.00000\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.8250e-05 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 1.00000\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.0531e-05 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 1.00000\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 3.0820e-05 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 1.00000\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.3024e-05 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 1.00000\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.1412e-05 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 1.00000\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.1598e-05 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 1.00000\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.6840e-05 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 1.00000\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.9541e-05 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 1.00000\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.2839e-05 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 1.00000\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.3992e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 1.00000\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.9932e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 1.00000\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.2166e-05 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 1.00000\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5061e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 1.00000\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.0313e-05 - acc: 1.0000 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 1.00000\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7678e-05 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 1.00000\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.9053e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 1.00000\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.6167e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 1.00000\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.9767e-05 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 1.00000\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7133e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 1.00000\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5717e-05 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 1.00000\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7794e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 1.00000\n",
      "Epoch 194/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.3092e-05 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 1.00000\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.7626e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00195: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.4211e-05 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 1.00000\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.9564e-05 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 1.00000\n",
      "Epoch 198/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 2.4435e-05 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 1.00000\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.5860e-05 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 1.00000\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 1.4659e-05 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[94,  0],\n",
       "        [ 0, 34]],\n",
       "\n",
       "       [[81,  0],\n",
       "        [ 0, 47]],\n",
       "\n",
       "       [[81,  0],\n",
       "        [ 0, 47]]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e84a031d95f7e8f7868565c0ce39ef3a08935952672a9a356404111c8a2bb977"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
